from sklearn.model_selection import train_test_split

# STEP 1: split X and y into training and testing sets (using random_state for reproducibility)
X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99, test_size=0.5)

# STEP 2: train the model on the training set (using K=1)
knn = KNeighborsClassifier(n_neighbors=1)
knn.fit(X_train, y_train)

# STEP 3: test the model on the testing set, and check the accuracy
from sklearn import metrics
y_pred_class = knn.predict(X_test)
print('accuracy = {}'.format(metrics.accuracy_score(y_test, y_pred_class)))



###  KNN with cross_val_score

Example 1:
Fit KNN across different values of K and plot the mean cross-validated accuracy with 5 folds.

from sklearn.model_selection import cross_val_score

folds = 5
max_neighbors = np.floor(X.shape[0] - X.shape[0]/5.)

print(max_neighbors)

### plot test accuracy by number of neighbors:
test_acc = []
for i in range(1, int(max_neighbors)):
    knn = KNeighborsClassifier(n_neighbors=i)
    test_acc.append(np.mean(cross_val_score(knn, X, y, cv=5)))
	
Example 2:
scores = cross_val_score(knn, Xs, y, cv=10)
print(scores)
print(np.mean(scores))



### Function to find the best K for predicting
def find_best_k_cls(X, y, k_min=1, k_max=51, step=2, cv=5):
    k_range = range(k_min, k_max+1, step)
    accs = []
    for k in k_range:
        knn = KNeighborsClassifier(n_neighbors=k)
        scores = cross_val_score(knn, X, y, cv=cv)
        accs.append(np.mean(scores))
    print(np.max(accs), np.argmax(k_range))
    return np.argmax(k_range)
	
	
	
	
StratifiedKFold
### Create the Cross-Validation Indices Using StratifiedKFold
skf = StratifiedKFold(n_splits=5)
cv_indices = skf.split(Xs, y)
cv_indices = [[train,test] for train,test in cv_indices]


Write a Function to Manually Perform Cross-Validation Using Your Stratified Indices
Now that we have the indices — row indexes for our train/test splits — write a function that will:

Split the x and y into training and testing subsets.
Fit a KNN classifier on the training set.
Calculate the accuracy of the classifier on the testing set.
Store the accuracies for each fold and return them as a list.


### Function to crossvalidate accuracy of a knn model across folds
def accuracy_crossvalidator(X, y, knn, cv_indices):
    
    # list to store the scores/accuracy of folds
    scores = []
    
    # iterate through the training and testing index folds in cv_indices
    for train_i, test_i in cv_indices:
        
        # get the current X train & test subsets of X
        X_train = X[train_i, :]
        X_test = X[test_i, :]

        # get the Y train & test subsets of Y
        Y_train = y[train_i]
        Y_test = y[test_i]

        # fit the knn model on the training data
        knn.fit(X_train, Y_train)
        
        # get the accuracy predicting the testing data
        acc = knn.score(X_test, Y_test)
        scores.append(acc)
        
        print('Fold accuracy:', acc)
        
    print('Mean CV accuracy:', np.mean(scores))
    return scores
				
	
## Cross-Validate the Mean Accuracy for a KNN Model With Five Neighbors

scores = accuracy_crossvalidator(Xs, y, knn5, cv_indices)
knn5 = KNeighborsClassifier(n_neighbors=5, weights='uniform')
​
scores = accuracy_crossvalidator(Xs, y, knn5, cv_indices)